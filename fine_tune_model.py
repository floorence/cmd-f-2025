# -*- coding: utf-8 -*-
"""fine_tune_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aGihqQSeaWd8tzmHZldTh6Svrc-BsKlE
"""

!pip install huggingface_sb3
!pip install huggingface_hub
!pip install panda_gym
!pip install datasets

import pandas as pd

# Load the dataset
df = pd.read_csv("/content/health_data_final.csv")

# Preview the first few rows
print(df.head())

from sklearn.model_selection import train_test_split

# 80% training, 20% testing
train_df, test_df = train_test_split(df, test_size=0.3, random_state=42)

# Save new CSV files
train_df.to_csv("/content/processed_train.csv", index=False)
test_df.to_csv("/content/processed_test.csv", index=False)

print("Data split successfully!")

from datasets import load_dataset

# Load the split dataset into Hugging Face's format
dataset = load_dataset("csv", data_files={"train": "/content/processed_train.csv",
                                          "test": "/content/processed_test.csv"})

# Preview a sample from the training set
print(dataset["train"][0])

from transformers import AutoTokenizer, AutoModelForSequenceClassification
from google.colab import userdata
from huggingface_hub import login

hugging_face_token = userdata.get('hugging_face_token')
login(token=hugging_face_token)

# tokenizing data

from transformers import AutoTokenizer

MODEL_NAME = "ungjus/Fake_News_BERT_Classifier"
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME) # loads tokenizer corresponding to the chosen model - helps convert raw text into numerical format (tokens) for the model

# this function tokenizes test data, uses padding to ensure all inputs have the same length, uses truncation to cut off long texts beyond model's limit
def tokenize_function(examples):
    return tokenizer(examples["text"], padding="max_length", truncation=True, max_length=512) # Added padding and max_length

tokenized_datasets = dataset.map(tokenize_function, batched=True) #applies the tokenize_function to every example in the dataset

print(tokenized_datasets["train"][0])  # Preview first training sample

from transformers import TrainerCallback, Trainer
import numpy as np

# Define Early Stopping Callback
class EarlyStoppingCallback(TrainerCallback):
    def __init__(self, patience=1, min_delta=0.01):
        self.patience = patience
        self.min_delta = min_delta
        self.best_loss = np.inf
        self.counter = 0

    def on_evaluate(self, args, state, control, logs=None, **kwargs):
        val_loss = logs["eval_loss"]
        if val_loss < self.best_loss - self.min_delta:
            self.best_loss = val_loss
            self.counter = 0
        else:
            self.counter += 1
            if self.counter >= self.patience:
                print("ðŸ”¥ Early stopping triggered: Stopping training!")
                control.should_training_stop = True

from transformers import (AutoModelForSequenceClassification, TrainingArguments, Trainer,
                          get_scheduler, AutoConfig)

from datasets import Dataset
from torch.optim import AdamW

# Ensure dataset contains 'label' instead of 'class'
if "class" in tokenized_datasets["train"].column_names:
    tokenized_datasets = tokenized_datasets.rename_column("class", "label")


print(tokenized_datasets["train"][0])

# Convert dataset to Hugging Face Dataset format
train_dataset = tokenized_datasets["train"]
eval_dataset = tokenized_datasets["test"]

MODEL_NAME = "ungjus/Fake_News_BERT_Classifier"


config = AutoConfig.from_pretrained(MODEL_NAME)
config.num_labels = 2  # binary classification
config.hidden_dropout_prob = 0.2
config.attention_probs_dropout_prob = 0.2

model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, config=config)


# Define Training Arguments
training_args = TrainingArguments(
    output_dir="./results",
    evaluation_strategy="epoch",     # Evaluate after each epoch
    save_strategy="epoch",           # Save model checkpoint each epoch
    per_device_train_batch_size=4,   # Adjust based on GPU memory
    per_device_eval_batch_size=4,
    num_train_epochs=5,              # Increase if you have more data
    learning_rate=1e-6,              # Typical fine-tuning LR
    weight_decay=0.001,               # L2 regularization
    logging_dir="./logs",
    logging_steps=10,                # Log every N steps
    load_best_model_at_end=True,     # Optional: automatically load best checkpoint
    metric_for_best_model="eval_loss"
)

optimizer = AdamW(
    model.parameters(),
    lr=1e-6,
    betas=(0.9, 0.999),   # HPC-likes: betas
    eps=1e-08,            # HPC-likes: epsilon
    weight_decay=0.001
)

num_training_steps = len(train_dataset) // training_args.per_device_train_batch_size * training_args.num_train_epochs
lr_scheduler = get_scheduler(
    name="linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    optimizers=(optimizer, lr_scheduler)  # pass manual optimizer & scheduler
)

trainer.train()



prediction_output = trainer.predict(eval_dataset)
logits = prediction_output.predictions
true_labels = prediction_output.label_ids

# 2. Convert logits to predicted classes
#    (For a binary classification task, we pick the index of the higher logit)
predictions = np.argmax(logits, axis=-1)

# 3. Compute accuracy
accuracy = (predictions == true_labels).mean()

print(f"Accuracy: {accuracy:.4f}")

model.save_pretrained("saved_model")
tokenizer.save_pretrained("saved_model")